{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning openai diffusion model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "avkq78LOjVhV",
        "3Ol_0nghwwGC",
        "1NZ2Yi2CxITo",
        "CiMreX-_n6Kz"
      ],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajavid34/guided-diffusion-sxela/blob/main/fine_tuning_openai_diffusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple colab to fine-tune openai diffusion models.\n",
        "\n",
        "\n",
        "Feel free to ask questions in this post's comments: https://www.patreon.com/posts/66246423\n",
        "\n",
        "by [Alex Spirin](https://twitter.com/devdef)\n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=sxela_finetune_openai_colab)"
      ],
      "metadata": {
        "id": "3hBAjQO1kiEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup (run once per session)\n",
        "\n",
        "This mounts your google drive for easier storage"
      ],
      "metadata": {
        "id": "ufaUo7olwoF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EtMv2MEzSzjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "820ca579-f9d2-4441-cf69-1f670349ccaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the training code and installs it"
      ],
      "metadata": {
        "id": "Eg3mlCMIe1B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/ajavid34/guided-diffusion-sxela\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "yXvOPC8PfnUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4fdb11-1601-4af5-8a8d-b87ba2defb92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'guided-diffusion-sxela'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 157 (delta 10), reused 0 (delta 0), pack-reused 135 (from 1)\u001b[K\n",
            "Receiving objects: 100% (157/157), 120.08 KiB | 17.15 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n",
            "/content/guided-diffusion-sxela\n",
            "Obtaining file:///content/guided-diffusion-sxela\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: blobfile>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from guided-diffusion==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from guided-diffusion==0.0.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from guided-diffusion==0.0.0) (4.67.1)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (5.4.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->guided-diffusion==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->guided-diffusion==0.0.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, guided-diffusion\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Running setup.py develop for guided-diffusion\n",
            "Successfully installed guided-diffusion-0.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train (tune) BEDROOM model :D\n",
        "Needs 16gb GPU RAM\n",
        "\n",
        "Works in colab pro and on kaggle"
      ],
      "metadata": {
        "id": "JZ8BNzApp_Xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a pre-trained LSUN BEDROOM model that we will be tuning on our dataset"
      ],
      "metadata": {
        "id": "RmI7jtj5fzJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt -P /content/"
      ],
      "metadata": {
        "id": "h-fL3fb8wpxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ab6a99-e031-4e9a-e782-f991771175cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-28 21:53:03--  https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 454799209 (434M) [application/octet-stream]\n",
            "Saving to: ‘/content/lsun_uncond_100M_1200K_bs128.pt’\n",
            "\n",
            "lsun_uncond_100M_12 100%[===================>] 433.73M  3.11MB/s    in 2m 20s  \n",
            "\n",
            "2025-05-28 21:55:24 (3.09 MB/s) - ‘/content/lsun_uncond_100M_1200K_bs128.pt’ saved [454799209/454799209]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Downloading and preparing dataset...\"\n",
        "!# Download Oxford 102 flowers dataset\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "\n",
        "# Extract and organize images\n",
        "!tar -xzf 102flowers.tgz\n",
        "!mkdir -p your_images\n",
        "!cp jpg/*.jpg your_images/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwwrAGcWYfK",
        "outputId": "75e11143-c8c4-4cc5-c518-f727b1d87359"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset...\n",
            "--2025-05-28 21:55:24--  https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz [following]\n",
            "--2025-05-28 21:55:25--  https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344862509 (329M) [application/octet-stream]\n",
            "Saving to: ‘102flowers.tgz’\n",
            "\n",
            "102flowers.tgz      100%[===================>] 328.89M  17.7MB/s    in 21s     \n",
            "\n",
            "2025-05-28 21:55:47 (16.0 MB/s) - ‘102flowers.tgz’ saved [344862509/344862509]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune"
      ],
      "metadata": {
        "id": "OV2gIxZhw2me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For gigachads.\n",
        "We're going to do what's called a pro-gamer move (or not): tune a small model, trained on bedrooms, on our own dataset. Just because we can and it's much faster than training from scratch.\n",
        "\n",
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to.\n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here\n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), and set width_height to 256x256, then run DD as usual\n"
      ],
      "metadata": {
        "id": "WqBBkqPjqESf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 2000 --log_interval 50 --resume_checkpoint /content/lsun_uncond_100M_1200K_bs128.pt\"\n",
        "DATASET_PATH=\"./your_images/\" #change to point to your dataset path\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\" #models will be saved here, change to your drive folder or else\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=$OUTPUT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ],
      "metadata": {
        "id": "6oddfFz71MGD",
        "outputId": "891adfa1-3180-4c1e-d477-a7e34af88b9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/guided-diffusion-sxela\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:143: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_fwd\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:153: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_bwd\n",
            "set output to  /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\n",
            "Logging to /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\n",
            "creating model and diffusion...\n",
            "creating data loader...\n",
            "training...\n",
            "loading model from checkpoint: /content/lsun_uncond_100M_1200K_bs128.pt...\n",
            "-------------------------\n",
            "| grad_norm  | 0.0195   |\n",
            "| loss       | 0.00481  |\n",
            "| loss_q1    | 0.00847  |\n",
            "| loss_q2    | 0.00115  |\n",
            "| mse        | 0.00477  |\n",
            "| mse_q1     | 0.00841  |\n",
            "| mse_q2     | 0.00114  |\n",
            "| param_norm | 683      |\n",
            "| samples    | 4        |\n",
            "| step       | 0        |\n",
            "| vb         | 3.6e-05  |\n",
            "| vb_q1      | 6.12e-05 |\n",
            "| vb_q2      | 1.08e-05 |\n",
            "-------------------------\n",
            "saving model 0...\n",
            "saving model 0.9999...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/guided-diffusion-sxela/scripts/image_train.py\", line 86, in <module>\n",
            "    main()\n",
            "  File \"/content/guided-diffusion-sxela/scripts/image_train.py\", line 59, in main\n",
            "    ).run_loop()\n",
            "      ^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/train_util.py\", line 153, in run_loop\n",
            "    self.run_step(batch, cond)\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/train_util.py\", line 168, in run_step\n",
            "    took_step = self.mp_trainer.optimize(self.opt)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/fp16_util.py\", line 187, in optimize\n",
            "    return self._optimize_normal(opt)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/fp16_util.py\", line 211, in _optimize_normal\n",
            "    grad_norm, param_norm = self._compute_norms()\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/fp16_util.py\", line 222, in _compute_norms\n",
            "    param_norm += th.norm(p, p=2, dtype=th.float32).item() ** 2\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier architecture flags\n",
        "CLASSIFIER_FLAGS=\"--image_size 256 --classifier_attention_resolutions 32,16,8 --classifier_depth 2 --classifier_width 128 --classifier_pool attention --classifier_use_fp16 True\"\n",
        "\n",
        "# Training flags\n",
        "TRAIN_FLAGS=\"--lr 3e-4 --batch_size 16 --save_interval 500 --log_interval 100 --iterations 1500 --anneal_lr True --weight_decay 0.05\"\n",
        "\n",
        "# ECT (Entropy-Constraint Training) flags\n",
        "ECT_FLAGS=\"--ect_weight 0.1 --ect_divergence JS --mi_weight 0.01 --mi_divergence JS\"\n",
        "\n",
        "# Entropy configuration flags\n",
        "ENTROPY_FLAGS=\"--entropy_type renyi --entropy_alpha 2.0\"\n",
        "\n",
        "# Dataset and paths\n",
        "DATASET_PATH=\"./your_images/\"  # Your ImageNet or dataset path\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-ect/\"  # Output directory\n",
        "\n",
        "# For training the noise-aware classifier with ECT\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!python scripts/classifier_train.py \\\n",
        "    --data_dir $DATASET_PATH \\\n",
        "    --noised True \\\n",
        "    $CLASSIFIER_FLAGS \\\n",
        "    $TRAIN_FLAGS \\\n",
        "    $ECT_FLAGS \\\n",
        "    $ENTROPY_FLAGS"
      ],
      "metadata": {
        "id": "apH5i0hTqz1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8c711c-4b6b-4d28-f65f-85005e0547b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/guided-diffusion-sxela\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:143: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_fwd\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:153: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_bwd\n",
            "Logging to /tmp/openai-2025-05-28-22-11-48-458238\n",
            "creating model and diffusion...\n",
            "creating data loader...\n",
            "creating optimizer...\n",
            "training classifier model with ECT...\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/guided-diffusion-sxela/scripts/classifier_train.py\", line 355, in <module>\n",
            "[rank0]:     main()\n",
            "[rank0]:   File \"/content/guided-diffusion-sxela/scripts/classifier_train.py\", line 261, in main\n",
            "[rank0]:     forward_backward_log(data)\n",
            "[rank0]:   File \"/content/guided-diffusion-sxela/scripts/classifier_train.py\", line 244, in forward_backward_log\n",
            "[rank0]:     log_loss_dict(diffusion, sub_t, losses)\n",
            "[rank0]:   File \"/content/guided-diffusion-sxela/guided_diffusion/train_util.py\", line 293, in log_loss_dict\n",
            "[rank0]:     for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n",
            "[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: TypeError: iteration over a 0-d array\n",
            "[rank0]:[W528 22:11:52.748561959 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "id": "hcYq1vz62yPo",
        "outputId": "a90ec40d-db6f-4597-de7c-dfe6fdae1907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4441922 sha256=63d173acfbce9d31b46990872502291690d26d76d34dda526c37d461aa638514\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion.\n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change model settings > custom model path to your ema checkpoint's location, as described in the previous cell."
      ],
      "metadata": {
        "id": "udICgtfHEiQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ],
      "metadata": {
        "id": "57cMKNlWF1VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "!python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS $DIFFUSION_FLAGS --timestep_respacing ddim100 --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS $DIFFUSION_FLAGS --timestep_respacing ddim100"
      ],
      "metadata": {
        "id": "O-RCVDtuGArz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ],
      "metadata": {
        "id": "nFPy3r8AGEW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train (tune) 256x256 vanilla DD model\n",
        "Only if you have a beefy GPU with more than 16gb RAM\n",
        "\n",
        "For lvl 50 AI bosses,\n",
        "Will not fit into colab pro, only in colab pro+ with A100 gpu\n"
      ],
      "metadata": {
        "id": "avkq78LOjVhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a pre-trained openai 256x256 model (the one used in DiscoDiffusion) that we will be tuning on our dataset"
      ],
      "metadata": {
        "id": "sk2sVBAxwurI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download model checkpoint\n",
        "!wget https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt -P /content/\n",
        "#if you wish to tune the 512x512 finetuned model from DD, you need to download it and change image size and checkpoint path later here:\n",
        "#!wget https://huggingface.co/lowlevelware/512x512_diffusion_unconditional_ImageNet/resolve/main/512x512_diffusion_uncond_finetune_008100.pt"
      ],
      "metadata": {
        "id": "Jjf4ZopAwwyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune"
      ],
      "metadata": {
        "id": "3Ol_0nghwwGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to.\n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here\n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there),\n",
        "\n",
        "you'll need to set custom model settings to this:\n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ],
      "metadata": {
        "id": "1_xc7GvAwgNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50 --resume_checkpoint /content/256x256_diffusion_uncond.pt\"\n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion/\"\n",
        "%cd /content/guided-diffusion\n",
        "!python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "# !OPENAI_LOGDIR=$OUTPUT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ],
      "metadata": {
        "id": "fJtcF4C_jDjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample from model"
      ],
      "metadata": {
        "id": "AHbxCkynj2h0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion.\n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ],
      "metadata": {
        "id": "1NZ2Yi2CxITo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ],
      "metadata": {
        "id": "_fD1dA5vxRDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "!python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100 --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100"
      ],
      "metadata": {
        "id": "tAZ1CwLkj11s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show results"
      ],
      "metadata": {
        "id": "l3cMMZLKkatO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ],
      "metadata": {
        "id": "_WGeIjHhkbnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train from scratch (smaller model than vanilla DD, but larger than LSUN)\n",
        "For lvl 1 AI crooks like me, should fit into colab pro"
      ],
      "metadata": {
        "id": "CiMreX-_n6Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a smaller model that will fit definitely into colab pro."
      ],
      "metadata": {
        "id": "MLR2sbXSoNdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to.\n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here\n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there),\n",
        "\n",
        "you'll need to set custom model settings to this:\n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 128,\n",
        "        'num_heads': 4,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ],
      "metadata": {
        "id": "3p6ThbjFxtBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 128 --num_heads 4  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50\"\n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "# !OPENAI_LOGDIR=$OUTPUT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ],
      "metadata": {
        "id": "UfH7XSbKn7ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion.\n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ],
      "metadata": {
        "id": "8seIEPF9pF7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "!python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS  --timestep_respacing ddim100 --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS  --timestep_respacing ddim100"
      ],
      "metadata": {
        "id": "GaHnukcKpEX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show results"
      ],
      "metadata": {
        "id": "3mfZb81vpIK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ],
      "metadata": {
        "id": "QCwCF0NhpHy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}