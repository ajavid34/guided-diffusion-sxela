{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning openai diffusion model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "avkq78LOjVhV",
        "3Ol_0nghwwGC",
        "1NZ2Yi2CxITo",
        "CiMreX-_n6Kz"
      ],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajavid34/guided-diffusion-sxela/blob/main/fine_tuning_openai_diffusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple colab to fine-tune openai diffusion models.\n",
        "\n",
        "\n",
        "Feel free to ask questions in this post's comments: https://www.patreon.com/posts/66246423\n",
        "\n",
        "by [Alex Spirin](https://twitter.com/devdef)\n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=sxela_finetune_openai_colab)"
      ],
      "metadata": {
        "id": "3hBAjQO1kiEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup (run once per session)\n",
        "\n",
        "This mounts your google drive for easier storage"
      ],
      "metadata": {
        "id": "ufaUo7olwoF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EtMv2MEzSzjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b357d45-2d0b-42e0-dce1-2f6429b3166a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the training code and installs it"
      ],
      "metadata": {
        "id": "Eg3mlCMIe1B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/ajavid34/guided-diffusion-sxela\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "yXvOPC8PfnUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1e1397-5831-41a5-a86d-6cb6737943d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'guided-diffusion-sxela'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 176 (delta 11), reused 3 (delta 3), pack-reused 156 (from 2)\u001b[K\n",
            "Receiving objects: 100% (176/176), 137.94 KiB | 1.84 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "/content/guided-diffusion-sxela\n",
            "Obtaining file:///content/guided-diffusion-sxela\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: blobfile>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from guided-diffusion==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from guided-diffusion==0.0.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from guided-diffusion==0.0.0) (4.67.1)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (5.4.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->guided-diffusion==0.0.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->guided-diffusion==0.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->guided-diffusion==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->guided-diffusion==0.0.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, guided-diffusion\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Running setup.py develop for guided-diffusion\n",
            "Successfully installed guided-diffusion-0.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train (tune) BEDROOM model :D\n",
        "Needs 16gb GPU RAM\n",
        "\n",
        "Works in colab pro and on kaggle"
      ],
      "metadata": {
        "id": "JZ8BNzApp_Xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a pre-trained LSUN BEDROOM model that we will be tuning on our dataset"
      ],
      "metadata": {
        "id": "RmI7jtj5fzJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion.pt -P /content/"
      ],
      "metadata": {
        "id": "h-fL3fb8wpxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445acf38-9bc7-4a6c-e5e6-43a8bb755265"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-29 01:59:48--  https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion.pt\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2215479544 (2.1G) [application/octet-stream]\n",
            "Saving to: ‘/content/256x256_diffusion.pt’\n",
            "\n",
            "256x256_diffusion.p 100%[===================>]   2.06G  23.8MB/s    in 98s     \n",
            "\n",
            "2025-05-29 02:01:26 (21.5 MB/s) - ‘/content/256x256_diffusion.pt’ saved [2215479544/2215479544]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Downloading and preparing dataset...\"\n",
        "!# Download Oxford 102 flowers dataset\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "\n",
        "# Extract and organize images\n",
        "!tar -xzf 102flowers.tgz\n",
        "!mkdir -p your_images\n",
        "!cp jpg/*.jpg your_images/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwwrAGcWYfK",
        "outputId": "f92451e2-c0d1-40e4-8f5e-69643b943924"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset...\n",
            "--2025-05-29 02:01:27--  https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz [following]\n",
            "--2025-05-29 02:01:27--  https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344862509 (329M) [application/octet-stream]\n",
            "Saving to: ‘102flowers.tgz’\n",
            "\n",
            "102flowers.tgz      100%[===================>] 328.89M  30.9MB/s    in 11s     \n",
            "\n",
            "2025-05-29 02:01:39 (28.8 MB/s) - ‘102flowers.tgz’ saved [344862509/344862509]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune"
      ],
      "metadata": {
        "id": "OV2gIxZhw2me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For gigachads.\n",
        "We're going to do what's called a pro-gamer move (or not): tune a small model, trained on bedrooms, on our own dataset. Just because we can and it's much faster than training from scratch.\n",
        "\n",
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to.\n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here\n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), and set width_height to 256x256, then run DD as usual\n"
      ],
      "metadata": {
        "id": "WqBBkqPjqESf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 2000 --log_interval 50 --resume_checkpoint /content/lsun_uncond_100M_1200K_bs128.pt\"\n",
        "DATASET_PATH=\"./your_images/\" #change to point to your dataset path\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\" #models will be saved here, change to your drive folder or else\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=$OUTPUT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oddfFz71MGD",
        "outputId": "891adfa1-3180-4c1e-d477-a7e34af88b9f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/guided-diffusion-sxela\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:143: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_fwd\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:153: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_bwd\n",
            "set output to  /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\n",
            "Logging to /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\n",
            "creating model and diffusion...\n",
            "creating data loader...\n",
            "training...\n",
            "loading model from checkpoint: /content/lsun_uncond_100M_1200K_bs128.pt...\n",
            "-------------------------\n",
            "| grad_norm  | 0.0195   |\n",
            "| loss       | 0.00481  |\n",
            "| loss_q1    | 0.00847  |\n",
            "| loss_q2    | 0.00115  |\n",
            "| mse        | 0.00477  |\n",
            "| mse_q1     | 0.00841  |\n",
            "| mse_q2     | 0.00114  |\n",
            "| param_norm | 683      |\n",
            "| samples    | 4        |\n",
            "| step       | 0        |\n",
            "| vb         | 3.6e-05  |\n",
            "| vb_q1      | 6.12e-05 |\n",
            "| vb_q2      | 1.08e-05 |\n",
            "-------------------------\n",
            "saving model 0...\n",
            "saving model 0.9999...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/guided-diffusion-sxela/scripts/image_train.py\", line 86, in <module>\n",
            "    main()\n",
            "  File \"/content/guided-diffusion-sxela/scripts/image_train.py\", line 59, in main\n",
            "    ).run_loop()\n",
            "      ^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/train_util.py\", line 153, in run_loop\n",
            "    self.run_step(batch, cond)\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/train_util.py\", line 168, in run_step\n",
            "    took_step = self.mp_trainer.optimize(self.opt)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/fp16_util.py\", line 187, in optimize\n",
            "    return self._optimize_normal(opt)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/fp16_util.py\", line 211, in _optimize_normal\n",
            "    grad_norm, param_norm = self._compute_norms()\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/fp16_util.py\", line 222, in _compute_norms\n",
            "    param_norm += th.norm(p, p=2, dtype=th.float32).item() ** 2\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set explicit output directory\n",
        "OUTPUT_DIR=\"/content/classifier_ect_models\"\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "# Classifier architecture flags\n",
        "CLASSIFIER_FLAGS=\"--image_size 256 --classifier_attention_resolutions 32,16,8 --classifier_depth 2 --classifier_width 128 --classifier_pool attention --classifier_use_fp16 True\"\n",
        "\n",
        "# Training flags - quick run to test saving\n",
        "TRAIN_FLAGS=\"--lr 1e-4 --batch_size 32 --save_interval 200 --log_interval 100 --iterations 3000 --anneal_lr True --weight_decay 0.05\"\n",
        "\n",
        "# ECT flags\n",
        "ECT_FLAGS=\"--ect_weight 0.1 --ect_divergence JS --mi_weight 0.01 --mi_divergence JS\"\n",
        "\n",
        "# Entropy configuration flags\n",
        "ENTROPY_FLAGS=\"--entropy_type renyi --entropy_alpha 2.0\"\n",
        "\n",
        "# Dataset path\n",
        "DATASET_PATH=\"./your_images/\"\n",
        "\n",
        "# Run training with explicit logging directory\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$OUTPUT_DIR python scripts/classifier_train.py \\\n",
        "    --data_dir $DATASET_PATH \\\n",
        "    --noised True \\\n",
        "    $CLASSIFIER_FLAGS \\\n",
        "    $TRAIN_FLAGS \\\n",
        "    $ECT_FLAGS \\\n",
        "    $ENTROPY_FLAGS"
      ],
      "metadata": {
        "id": "apH5i0hTqz1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13520d4c-38f0-4235-d4c4-b3015284e412"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/guided-diffusion-sxela\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:143: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_fwd\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:153: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_bwd\n",
            "Logging to /content/classifier_ect_models\n",
            "creating model and diffusion...\n",
            "creating data loader...\n",
            "creating optimizer...\n",
            "training classifier model with ECT...\n",
            "Found NaN, decreased lg_loss_scale to 15.0\n",
            "--------------------------------\n",
            "| lg_loss_scale    | 16        |\n",
            "| samples          | 32        |\n",
            "| step             | 0         |\n",
            "| train_acc@1      | 0         |\n",
            "| train_acc@1_q0   | 0         |\n",
            "| train_acc@1_q1   | 0         |\n",
            "| train_acc@1_q2   | 0         |\n",
            "| train_acc@1_q3   | 0         |\n",
            "| train_acc@5      | 0         |\n",
            "| train_acc@5_q0   | 0         |\n",
            "| train_acc@5_q1   | 0         |\n",
            "| train_acc@5_q2   | 0         |\n",
            "| train_acc@5_q3   | 0         |\n",
            "| train_ect_loss   | -5.84e-08 |\n",
            "| train_entropy    | 6.87      |\n",
            "| train_loss       | 6.91      |\n",
            "| train_loss_q0    | 6.85      |\n",
            "| train_loss_q1    | 6.91      |\n",
            "| train_loss_q2    | 6.92      |\n",
            "| train_loss_q3    | 6.95      |\n",
            "| train_mi_loss    | 22.6      |\n",
            "| train_total_loss | 7.13      |\n",
            "--------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 4.41     |\n",
            "| lg_loss_scale    | 15       |\n",
            "| param_norm       | 160      |\n",
            "| samples          | 3.23e+03 |\n",
            "| step             | 100      |\n",
            "| train_acc@1      | 0.99     |\n",
            "| train_acc@1_q0   | 0.988    |\n",
            "| train_acc@1_q1   | 0.991    |\n",
            "| train_acc@1_q2   | 0.995    |\n",
            "| train_acc@1_q3   | 0.985    |\n",
            "| train_acc@5      | 0.99     |\n",
            "| train_acc@5_q0   | 0.988    |\n",
            "| train_acc@5_q1   | 0.991    |\n",
            "| train_acc@5_q2   | 0.995    |\n",
            "| train_acc@5_q3   | 0.985    |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 3.26e-06 |\n",
            "| train_loss       | 0.317    |\n",
            "| train_loss_q0    | 0.31     |\n",
            "| train_loss_q1    | 0.314    |\n",
            "| train_loss_q2    | 0.299    |\n",
            "| train_loss_q3    | 0.344    |\n",
            "| train_mi_loss    | 13.8     |\n",
            "| train_total_loss | 0.138    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 0.000157 |\n",
            "| lg_loss_scale    | 15.1     |\n",
            "| param_norm       | 160      |\n",
            "| samples          | 6.43e+03 |\n",
            "| step             | 200      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 4.31e-06 |\n",
            "| train_loss       | 3.21e-06 |\n",
            "| train_loss_q0    | 1.82e-07 |\n",
            "| train_loss_q1    | 1.06e-07 |\n",
            "| train_loss_q2    | 1.11e-06 |\n",
            "| train_loss_q3    | 1.09e-05 |\n",
            "| train_mi_loss    | 13.6     |\n",
            "| train_total_loss | 0.136    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model000200.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 0.000102 |\n",
            "| lg_loss_scale    | 15.2     |\n",
            "| param_norm       | 160      |\n",
            "| samples          | 9.63e+03 |\n",
            "| step             | 300      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 3.65e-06 |\n",
            "| train_loss       | 2e-06    |\n",
            "| train_loss_q0    | 6.43e-08 |\n",
            "| train_loss_q1    | 7.3e-08  |\n",
            "| train_loss_q2    | 5.47e-07 |\n",
            "| train_loss_q3    | 7.21e-06 |\n",
            "| train_mi_loss    | 14.2     |\n",
            "| train_total_loss | 0.142    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 0.000124 |\n",
            "| lg_loss_scale    | 15.3     |\n",
            "| param_norm       | 160      |\n",
            "| samples          | 1.28e+04 |\n",
            "| step             | 400      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 2.02e-05 |\n",
            "| train_loss       | 2.57e-06 |\n",
            "| train_loss_q0    | 1.58e-07 |\n",
            "| train_loss_q1    | 1.87e-07 |\n",
            "| train_loss_q2    | 6.72e-07 |\n",
            "| train_loss_q3    | 9.41e-06 |\n",
            "| train_mi_loss    | 13.5     |\n",
            "| train_total_loss | 0.135    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model000400.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 0.000128 |\n",
            "| lg_loss_scale    | 15.4     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 1.6e+04  |\n",
            "| step             | 500      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 7.67e-07 |\n",
            "| train_loss       | 2.76e-06 |\n",
            "| train_loss_q0    | 1.38e-07 |\n",
            "| train_loss_q1    | 1.32e-07 |\n",
            "| train_loss_q2    | 1.03e-06 |\n",
            "| train_loss_q3    | 9.7e-06  |\n",
            "| train_mi_loss    | 14.1     |\n",
            "| train_total_loss | 0.141    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 8.61e-05 |\n",
            "| lg_loss_scale    | 15.5     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 1.92e+04 |\n",
            "| step             | 600      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 4.05e-06 |\n",
            "| train_loss       | 1.72e-06 |\n",
            "| train_loss_q0    | 1.3e-07  |\n",
            "| train_loss_q1    | 1.31e-07 |\n",
            "| train_loss_q2    | 6.68e-07 |\n",
            "| train_loss_q3    | 5.87e-06 |\n",
            "| train_mi_loss    | 14.2     |\n",
            "| train_total_loss | 0.142    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model000600.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 0.00012  |\n",
            "| lg_loss_scale    | 15.6     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 2.24e+04 |\n",
            "| step             | 700      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 1.09e-05 |\n",
            "| train_loss       | 2.41e-06 |\n",
            "| train_loss_q0    | 8.9e-08  |\n",
            "| train_loss_q1    | 8.6e-08  |\n",
            "| train_loss_q2    | 8.25e-07 |\n",
            "| train_loss_q3    | 8.56e-06 |\n",
            "| train_mi_loss    | 14.5     |\n",
            "| train_total_loss | 0.145    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 9.63e-05 |\n",
            "| lg_loss_scale    | 15.7     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 2.56e+04 |\n",
            "| step             | 800      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 8.64e-06 |\n",
            "| train_loss       | 1.93e-06 |\n",
            "| train_loss_q0    | 6.88e-08 |\n",
            "| train_loss_q1    | 1.36e-07 |\n",
            "| train_loss_q2    | 7.65e-07 |\n",
            "| train_loss_q3    | 6.87e-06 |\n",
            "| train_mi_loss    | 14.2     |\n",
            "| train_total_loss | 0.142    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model000800.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 8.42e-05 |\n",
            "| lg_loss_scale    | 15.8     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 2.88e+04 |\n",
            "| step             | 900      |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 6.71e-07 |\n",
            "| train_loss       | 1.61e-06 |\n",
            "| train_loss_q0    | 5.31e-08 |\n",
            "| train_loss_q1    | 9.9e-08  |\n",
            "| train_loss_q2    | 6.21e-07 |\n",
            "| train_loss_q3    | 5.67e-06 |\n",
            "| train_mi_loss    | 15.1     |\n",
            "| train_total_loss | 0.151    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 8.73e-05 |\n",
            "| lg_loss_scale    | 15.9     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 3.2e+04  |\n",
            "| step             | 1e+03    |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 7.82e-07 |\n",
            "| train_loss       | 1.73e-06 |\n",
            "| train_loss_q0    | 9.9e-08  |\n",
            "| train_loss_q1    | 1.4e-07  |\n",
            "| train_loss_q2    | 8.02e-07 |\n",
            "| train_loss_q3    | 6.06e-06 |\n",
            "| train_mi_loss    | 14.4     |\n",
            "| train_total_loss | 0.144    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model001000.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 0.000119 |\n",
            "| lg_loss_scale    | 16       |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 3.52e+04 |\n",
            "| step             | 1.1e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 1.08e-05 |\n",
            "| train_loss       | 2.37e-06 |\n",
            "| train_loss_q0    | 1.57e-07 |\n",
            "| train_loss_q1    | 1.12e-07 |\n",
            "| train_loss_q2    | 3.59e-07 |\n",
            "| train_loss_q3    | 8.46e-06 |\n",
            "| train_mi_loss    | 14.4     |\n",
            "| train_total_loss | 0.145    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 9.82e-05 |\n",
            "| lg_loss_scale    | 16.1     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 3.84e+04 |\n",
            "| step             | 1.2e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 8.72e-07 |\n",
            "| train_loss       | 1.98e-06 |\n",
            "| train_loss_q0    | 2.41e-07 |\n",
            "| train_loss_q1    | 8.04e-08 |\n",
            "| train_loss_q2    | 7.89e-07 |\n",
            "| train_loss_q3    | 6.41e-06 |\n",
            "| train_mi_loss    | 14.4     |\n",
            "| train_total_loss | 0.144    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model001200.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 8.75e-05 |\n",
            "| lg_loss_scale    | 16.2     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 4.16e+04 |\n",
            "| step             | 1.3e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 3.8e-07  |\n",
            "| train_loss       | 1.69e-06 |\n",
            "| train_loss_q0    | 4.35e-08 |\n",
            "| train_loss_q1    | 1.39e-07 |\n",
            "| train_loss_q2    | 5.49e-07 |\n",
            "| train_loss_q3    | 5.89e-06 |\n",
            "| train_mi_loss    | 14.7     |\n",
            "| train_total_loss | 0.147    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 6.86e-05 |\n",
            "| lg_loss_scale    | 16.3     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 4.48e+04 |\n",
            "| step             | 1.4e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 2.5e-06  |\n",
            "| train_loss       | 1.28e-06 |\n",
            "| train_loss_q0    | 2.89e-08 |\n",
            "| train_loss_q1    | 1.11e-07 |\n",
            "| train_loss_q2    | 5.24e-07 |\n",
            "| train_loss_q3    | 4.35e-06 |\n",
            "| train_mi_loss    | 14.8     |\n",
            "| train_total_loss | 0.148    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model001400.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 7.2e-05  |\n",
            "| lg_loss_scale    | 16.4     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 4.8e+04  |\n",
            "| step             | 1.5e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 4.02e-07 |\n",
            "| train_loss       | 1.33e-06 |\n",
            "| train_loss_q0    | 1.16e-07 |\n",
            "| train_loss_q1    | 6.69e-08 |\n",
            "| train_loss_q2    | 6.13e-07 |\n",
            "| train_loss_q3    | 4.49e-06 |\n",
            "| train_mi_loss    | 14.3     |\n",
            "| train_total_loss | 0.143    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 4.6e-05  |\n",
            "| lg_loss_scale    | 16.5     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 5.12e+04 |\n",
            "| step             | 1.6e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 5.29e-07 |\n",
            "| train_loss       | 8.75e-07 |\n",
            "| train_loss_q0    | 1.95e-07 |\n",
            "| train_loss_q1    | 1.3e-07  |\n",
            "| train_loss_q2    | 3.08e-07 |\n",
            "| train_loss_q3    | 2.88e-06 |\n",
            "| train_mi_loss    | 15.1     |\n",
            "| train_total_loss | 0.151    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model001600.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 5.48e-05 |\n",
            "| lg_loss_scale    | 16.6     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 5.44e+04 |\n",
            "| step             | 1.7e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 3.65e-07 |\n",
            "| train_loss       | 1.04e-06 |\n",
            "| train_loss_q0    | 9.61e-08 |\n",
            "| train_loss_q1    | 7.13e-08 |\n",
            "| train_loss_q2    | 4.37e-07 |\n",
            "| train_loss_q3    | 3.56e-06 |\n",
            "| train_mi_loss    | 14.3     |\n",
            "| train_total_loss | 0.143    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 4.99e-05 |\n",
            "| lg_loss_scale    | 16.7     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 5.76e+04 |\n",
            "| step             | 1.8e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 2.31e-07 |\n",
            "| train_loss       | 8.61e-07 |\n",
            "| train_loss_q0    | 6.76e-08 |\n",
            "| train_loss_q1    | 6.17e-08 |\n",
            "| train_loss_q2    | 4.45e-07 |\n",
            "| train_loss_q3    | 2.89e-06 |\n",
            "| train_mi_loss    | 14.5     |\n",
            "| train_total_loss | 0.145    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model001800.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 6.48e-05 |\n",
            "| lg_loss_scale    | 16.8     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 6.08e+04 |\n",
            "| step             | 1.9e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 1.57e-06 |\n",
            "| train_loss       | 1.21e-06 |\n",
            "| train_loss_q0    | 1.45e-07 |\n",
            "| train_loss_q1    | 7.23e-08 |\n",
            "| train_loss_q2    | 7.36e-07 |\n",
            "| train_loss_q3    | 3.85e-06 |\n",
            "| train_mi_loss    | 13.6     |\n",
            "| train_total_loss | 0.136    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 6.34e-05 |\n",
            "| lg_loss_scale    | 16.9     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 6.4e+04  |\n",
            "| step             | 2e+03    |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 5.22e-08 |\n",
            "| train_loss       | 1.15e-06 |\n",
            "| train_loss_q0    | 5.28e-08 |\n",
            "| train_loss_q1    | 1e-07    |\n",
            "| train_loss_q2    | 6.68e-07 |\n",
            "| train_loss_q3    | 3.66e-06 |\n",
            "| train_mi_loss    | 15.2     |\n",
            "| train_total_loss | 0.152    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model002000.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 6.69e-05 |\n",
            "| lg_loss_scale    | 17       |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 6.72e+04 |\n",
            "| step             | 2.1e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 3.05e-07 |\n",
            "| train_loss       | 1.18e-06 |\n",
            "| train_loss_q0    | 6.67e-08 |\n",
            "| train_loss_q1    | 1.33e-07 |\n",
            "| train_loss_q2    | 1.84e-07 |\n",
            "| train_loss_q3    | 4.22e-06 |\n",
            "| train_mi_loss    | 14.1     |\n",
            "| train_total_loss | 0.141    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 6.09e-05 |\n",
            "| lg_loss_scale    | 17.1     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 7.04e+04 |\n",
            "| step             | 2.2e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 2.23e-06 |\n",
            "| train_loss       | 1.13e-06 |\n",
            "| train_loss_q0    | 7.3e-08  |\n",
            "| train_loss_q1    | 6.46e-08 |\n",
            "| train_loss_q2    | 2.5e-07  |\n",
            "| train_loss_q3    | 4.1e-06  |\n",
            "| train_mi_loss    | 14.7     |\n",
            "| train_total_loss | 0.147    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model002200.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 3.88e-05 |\n",
            "| lg_loss_scale    | 17.2     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 7.36e+04 |\n",
            "| step             | 2.3e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 3.75e-06 |\n",
            "| train_loss       | 6.8e-07  |\n",
            "| train_loss_q0    | 1.38e-07 |\n",
            "| train_loss_q1    | 5.23e-08 |\n",
            "| train_loss_q2    | 2.99e-07 |\n",
            "| train_loss_q3    | 2.31e-06 |\n",
            "| train_mi_loss    | 14       |\n",
            "| train_total_loss | 0.14     |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 4.67e-05 |\n",
            "| lg_loss_scale    | 17.3     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 7.68e+04 |\n",
            "| step             | 2.4e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 2.98e-08 |\n",
            "| train_loss       | 7.95e-07 |\n",
            "| train_loss_q0    | 2.04e-08 |\n",
            "| train_loss_q1    | 1.05e-07 |\n",
            "| train_loss_q2    | 2.32e-07 |\n",
            "| train_loss_q3    | 2.84e-06 |\n",
            "| train_mi_loss    | 15.7     |\n",
            "| train_total_loss | 0.157    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model002400.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 5.43e-05 |\n",
            "| lg_loss_scale    | 17.4     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 8e+04    |\n",
            "| step             | 2.5e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 7.67e-07 |\n",
            "| train_loss       | 9.92e-07 |\n",
            "| train_loss_q0    | 4.31e-08 |\n",
            "| train_loss_q1    | 5.54e-08 |\n",
            "| train_loss_q2    | 1.57e-07 |\n",
            "| train_loss_q3    | 3.66e-06 |\n",
            "| train_mi_loss    | 13.9     |\n",
            "| train_total_loss | 0.139    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 4.12e-05 |\n",
            "| lg_loss_scale    | 17.5     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 8.32e+04 |\n",
            "| step             | 2.6e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 4.46e-06 |\n",
            "| train_loss       | 6.84e-07 |\n",
            "| train_loss_q0    | 1.22e-07 |\n",
            "| train_loss_q1    | 7.86e-08 |\n",
            "| train_loss_q2    | 2.91e-07 |\n",
            "| train_loss_q3    | 2.26e-06 |\n",
            "| train_mi_loss    | 14.6     |\n",
            "| train_total_loss | 0.146    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model002600.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 5.54e-05 |\n",
            "| lg_loss_scale    | 17.6     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 8.64e+04 |\n",
            "| step             | 2.7e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 8.27e-07 |\n",
            "| train_loss       | 9.78e-07 |\n",
            "| train_loss_q0    | 3.63e-08 |\n",
            "| train_loss_q1    | 1.3e-07  |\n",
            "| train_loss_q2    | 4.83e-07 |\n",
            "| train_loss_q3    | 3.28e-06 |\n",
            "| train_mi_loss    | 15.7     |\n",
            "| train_total_loss | 0.157    |\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "| grad_norm        | 4.33e-05 |\n",
            "| lg_loss_scale    | 17.7     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 8.96e+04 |\n",
            "| step             | 2.8e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 2.08e-06 |\n",
            "| train_loss       | 7.29e-07 |\n",
            "| train_loss_q0    | 2.16e-07 |\n",
            "| train_loss_q1    | 8.86e-08 |\n",
            "| train_loss_q2    | 1.94e-07 |\n",
            "| train_loss_q3    | 2.37e-06 |\n",
            "| train_mi_loss    | 14.8     |\n",
            "| train_total_loss | 0.148    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model002800.pt\n",
            "Model saved successfully!\n",
            "-------------------------------\n",
            "| grad_norm        | 3.1e-05  |\n",
            "| lg_loss_scale    | 17.8     |\n",
            "| param_norm       | 159      |\n",
            "| samples          | 9.28e+04 |\n",
            "| step             | 2.9e+03  |\n",
            "| train_acc@1      | 1        |\n",
            "| train_acc@1_q0   | 1        |\n",
            "| train_acc@1_q1   | 1        |\n",
            "| train_acc@1_q2   | 1        |\n",
            "| train_acc@1_q3   | 1        |\n",
            "| train_acc@5      | 1        |\n",
            "| train_acc@5_q0   | 1        |\n",
            "| train_acc@5_q1   | 1        |\n",
            "| train_acc@5_q2   | 1        |\n",
            "| train_acc@5_q3   | 1        |\n",
            "| train_ect_loss   | 0.000206 |\n",
            "| train_entropy    | 3.73e-08 |\n",
            "| train_loss       | 5.09e-07 |\n",
            "| train_loss_q0    | 3.81e-08 |\n",
            "| train_loss_q1    | 5.6e-08  |\n",
            "| train_loss_q2    | 4.62e-07 |\n",
            "| train_loss_q3    | 1.48e-06 |\n",
            "| train_mi_loss    | 15.8     |\n",
            "| train_total_loss | 0.158    |\n",
            "-------------------------------\n",
            "saving model...\n",
            "Saving model to /content/classifier_ect_models/model002999.pt\n",
            "Model saved successfully!\n",
            "Training completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check saved files\n",
        "OUTPUT_DIR=\"/content/classifier_ect_models\"\n",
        "!ls -la $OUTPUT_DIR/"
      ],
      "metadata": {
        "id": "6ePJa0QLrptE",
        "outputId": "da1be666-f1ff-4a0b-a950-ac8037345fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 9510024\n",
            "drwxr-xr-x 2 root root      4096 May 29 02:22 .\n",
            "drwxr-xr-x 1 root root      4096 May 29 02:05 ..\n",
            "-rw-r--r-- 1 root root     26714 May 29 02:22 log.txt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:06 model000200.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:07 model000400.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:08 model000600.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:10 model000800.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:11 model001000.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:12 model001200.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:13 model001400.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:14 model001600.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:15 model001800.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:16 model002000.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:18 model002200.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:19 model002400.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:20 model002600.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:21 model002800.pt\n",
            "-rw-r--r-- 1 root root 216436528 May 29 02:22 model002999.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:06 opt000200.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:07 opt000400.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:09 opt000600.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:10 opt000800.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:11 opt001000.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:12 opt001200.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:13 opt001400.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:14 opt001600.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:15 opt001800.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:16 opt002000.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:18 opt002200.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:19 opt002400.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:20 opt002600.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:21 opt002800.pt\n",
            "-rw-r--r-- 1 root root 432773596 May 29 02:22 opt002999.pt\n",
            "-rw-r--r-- 1 root root      8385 May 29 02:21 progress.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcYq1vz62yPo",
        "outputId": "e02247ae-efd1-451e-d804-451390ca1804"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/466.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4441920 sha256=8aaf6c2d0e83bf4056e29cccf9f706baf78b4e3f12066a4f2b0af662f9365be5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion.\n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change model settings > custom model path to your ema checkpoint's location, as described in the previous cell."
      ],
      "metadata": {
        "id": "udICgtfHEiQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ],
      "metadata": {
        "id": "57cMKNlWF1VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "!python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS $DIFFUSION_FLAGS --timestep_respacing ddim100 --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS $DIFFUSION_FLAGS --timestep_respacing ddim100"
      ],
      "metadata": {
        "id": "O-RCVDtuGArz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use the correct paths\n",
        "DIFFUSION_MODEL_PATH=\"/content/256x256_diffusion.pt\"  # Your unconditional diffusion model\n",
        "CLASSIFIER_PATH=\"/content/classifier_ect_models/model002999.pt\"  # Replace with actual saved classifier\n",
        "\n",
        "# Run sampling with unconditional diffusion model\n",
        "!python scripts/classifier_sample.py \\\n",
        "    --attention_resolutions 32,16,8 \\\n",
        "    --class_cond True \\\n",
        "    --diffusion_steps 3000 \\\n",
        "    --image_size 256 \\\n",
        "    --learn_sigma True \\\n",
        "    --noise_schedule linear \\\n",
        "    --num_channels 256 \\\n",
        "    --num_head_channels 64 \\\n",
        "    --num_res_blocks 2 \\\n",
        "    --resblock_updown True \\\n",
        "    --use_fp16 True \\\n",
        "    --use_scale_shift_norm True \\\n",
        "    --model_path $DIFFUSION_MODEL_PATH \\\n",
        "    --classifier_attention_resolutions 32,16,8 \\\n",
        "    --classifier_depth 2 \\\n",
        "    --classifier_width 128 \\\n",
        "    --classifier_pool attention \\\n",
        "    --classifier_resblock_updown True \\\n",
        "    --classifier_use_scale_shift_norm True \\\n",
        "    --classifier_use_fp16 True \\\n",
        "    --classifier_path $CLASSIFIER_PATH \\\n",
        "    --batch_size 32 \\\n",
        "    --num_samples 8 \\\n",
        "    --timestep_respacing 1000 \\\n",
        "    --classifier_scale 1.0 \\\n",
        "    --use_eds True \\\n",
        "    --entropy_type renyi \\\n",
        "    --entropy_alpha 2.0 \\\n",
        "    --entropy_weight 1.0 \\\n",
        "    --clip_denoised True"
      ],
      "metadata": {
        "id": "XgviMo--oqaL",
        "outputId": "a4bb8e83-caeb-450c-bf76-b4cc1bf44507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:143: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_fwd\n",
            "/content/guided-diffusion-sxela/guided_diffusion/nn.py:153: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @th.cuda.amp.custom_bwd\n",
            "Logging to /tmp/openai-2025-05-29-02-33-13-946820\n",
            "creating model and diffusion...\n",
            "loading classifier...\n",
            "sampling with Entropy-Driven Sampling...\n",
            "Using DDPM sampling\n",
            "100% 1000/1000 [18:42<00:00,  1.12s/it]\n",
            "created 32 samples\n",
            "saving to /tmp/openai-2025-05-29-02-33-13-946820/samples_8x256x256x3_eds_renyi_alpha2.0_gamma1.0.npz\n",
            "sampling complete\n",
            "[rank0]:[W529 02:52:10.383684131 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load the generated samples\n",
        "sample_path = \"/tmp/openai-2025-05-29-02-33-13-946820/samples_8x256x256x3_eds_renyi_alpha2.0_gamma1.0.npz\"\n",
        "data = np.load(sample_path)\n",
        "samples = data['arr_0']  # Images\n",
        "labels = data['arr_1']   # Class labels\n",
        "\n",
        "print(f\"Generated {samples.shape[0]} samples\")\n",
        "print(f\"Sample shape: {samples.shape}\")\n",
        "print(f\"Labels: {labels}\")\n",
        "\n",
        "# Visualize the samples\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(min(8, samples.shape[0])):\n",
        "    axes[i].imshow(samples[i])\n",
        "    axes[i].set_title(f\"Class: {labels[i]}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/eds_samples.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save individual images\n",
        "os.makedirs('/content/eds_samples', exist_ok=True)\n",
        "for i in range(samples.shape[0]):\n",
        "    img = Image.fromarray(samples[i])\n",
        "    img.save(f'/content/eds_samples/sample_{i}_class_{labels[i]}.png')"
      ],
      "metadata": {
        "id": "30o5G3_Jq8n0",
        "outputId": "cbf5c6dd-2685-43cc-cd90-304cd6bdff63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 8 samples\n",
            "Sample shape: (8, 256, 256, 3)\n",
            "Labels: [396 795 364 884 953 724 426 477]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(samples)"
      ],
      "metadata": {
        "id": "4QYr7k3aq8kx",
        "outputId": "8f315343-2a5c-4e9c-8842-2647cabc6324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ],
      "metadata": {
        "id": "nFPy3r8AGEW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train (tune) 256x256 vanilla DD model\n",
        "Only if you have a beefy GPU with more than 16gb RAM\n",
        "\n",
        "For lvl 50 AI bosses,\n",
        "Will not fit into colab pro, only in colab pro+ with A100 gpu\n"
      ],
      "metadata": {
        "id": "avkq78LOjVhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a pre-trained openai 256x256 model (the one used in DiscoDiffusion) that we will be tuning on our dataset"
      ],
      "metadata": {
        "id": "sk2sVBAxwurI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download model checkpoint\n",
        "!wget https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt -P /content/\n",
        "#if you wish to tune the 512x512 finetuned model from DD, you need to download it and change image size and checkpoint path later here:\n",
        "#!wget https://huggingface.co/lowlevelware/512x512_diffusion_unconditional_ImageNet/resolve/main/512x512_diffusion_uncond_finetune_008100.pt"
      ],
      "metadata": {
        "id": "Jjf4ZopAwwyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune"
      ],
      "metadata": {
        "id": "3Ol_0nghwwGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to.\n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here\n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there),\n",
        "\n",
        "you'll need to set custom model settings to this:\n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ],
      "metadata": {
        "id": "1_xc7GvAwgNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50 --resume_checkpoint /content/256x256_diffusion_uncond.pt\"\n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion/\"\n",
        "%cd /content/guided-diffusion\n",
        "!python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "# !OPENAI_LOGDIR=$OUTPUT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ],
      "metadata": {
        "id": "fJtcF4C_jDjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample from model"
      ],
      "metadata": {
        "id": "AHbxCkynj2h0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion.\n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ],
      "metadata": {
        "id": "1NZ2Yi2CxITo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ],
      "metadata": {
        "id": "_fD1dA5vxRDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "!python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100 --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100"
      ],
      "metadata": {
        "id": "tAZ1CwLkj11s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show results"
      ],
      "metadata": {
        "id": "l3cMMZLKkatO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ],
      "metadata": {
        "id": "_WGeIjHhkbnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train from scratch (smaller model than vanilla DD, but larger than LSUN)\n",
        "For lvl 1 AI crooks like me, should fit into colab pro"
      ],
      "metadata": {
        "id": "CiMreX-_n6Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a smaller model that will fit definitely into colab pro."
      ],
      "metadata": {
        "id": "MLR2sbXSoNdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to.\n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here\n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there),\n",
        "\n",
        "you'll need to set custom model settings to this:\n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 128,\n",
        "        'num_heads': 4,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ],
      "metadata": {
        "id": "3p6ThbjFxtBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 128 --num_heads 4  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50\"\n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "# !OPENAI_LOGDIR=$OUTPUT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ],
      "metadata": {
        "id": "UfH7XSbKn7ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion.\n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ],
      "metadata": {
        "id": "8seIEPF9pF7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "OUTPUT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "!python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS  --timestep_respacing ddim100 --logdir $OUTPUT_PATH\n",
        "\n",
        "#if you are using vanilla openai repo, then you will ned to run this:\n",
        "#!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS  --timestep_respacing ddim100"
      ],
      "metadata": {
        "id": "GaHnukcKpEX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show results"
      ],
      "metadata": {
        "id": "3mfZb81vpIK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ],
      "metadata": {
        "id": "QCwCF0NhpHy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}